#!groovy
// -*- mode: groovy -*-

// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

// Jenkins pipeline
// See documents at https://jenkins.io/doc/book/pipeline/jenkinsfile/

// Docker env used for testing
// Different image may have different version tag
// because some of them are more stable than anoter.
//
// Docker images are maintained by PMC, cached in dockerhub
// and remains relatively stable over the time.
// Flow for upgrading docker env(need commiter)
//
// - Send PR to upgrade build script in the repo
// - Build the new docker image
// - Tag the docker image with a new version and push to a binary cache.
// - Update the version in the Jenkinsfile, send a PR
// - Fix any issues wrt to the new image version in the PR
// - Merge the PR and now we are in new version
// - Tag the new version as the lates
// - Periodically cleanup the old versions on local workers
//

// ============================= IMPORTANT NOTE =============================
// This file is generated by 'jenkins/generate.py'. Do not edit this file directly!
// Make edits to 'jenkins/Jenkinsfile.j2' and regenerate this with
// 'python3 jenkins/generate.py'
// Note: This timestamp is here to ensure that updates to the Jenkinsfile are
// always rebased on main before merging:
// Generated at 2022-08-11T13:17:04.679404

import org.jenkinsci.plugins.pipeline.modeldefinition.Utils
// NOTE: these lines are scanned by docker/dev_common.sh. Please update the regex as needed. -->
ci_lint = 'tlcpack/ci-lint:v0.69'
ci_gpu = 'tlcpack/ci-gpu:v0.82'
ci_cpu = 'tlcpack/ci-cpu:v0.82'
ci_wasm = 'tlcpack/ci-wasm:v0.72'
ci_i386 = 'tlcpack/ci-i386:v0.75'
ci_qemu = 'tlcpack/ci-qemu:v0.11'
ci_arm = 'tlcpack/ci-arm:v0.08'
ci_hexagon = 'tlcpack/ci-hexagon:v0.02'
// <--- End of regex-scanned config.

// Parameters to allow overriding (in Jenkins UI), the images
// to be used by a given build. When provided, they take precedence
// over default values above.
properties([
  parameters([
    string(name: 'ci_arm_param', defaultValue: ''),
    string(name: 'ci_cpu_param', defaultValue: ''),
    string(name: 'ci_minimal_param', defaultValue: ''),
    string(name: 'ci_gpu_param', defaultValue: ''),
    string(name: 'ci_hexagon_param', defaultValue: ''),
    string(name: 'ci_i386_param', defaultValue: ''),
    string(name: 'ci_lint_param', defaultValue: ''),
    string(name: 'ci_cortexm_param', defaultValue: ''),
    string(name: 'ci_wasm_param', defaultValue: ''),
  ])
])

// Placeholders for newly built Docker image names (if rebuild_docker_images
// is used)
  built_ci_arm = null;
  built_ci_cpu = null;
  built_ci_minimal = null;
  built_ci_gpu = null;
  built_ci_hexagon = null;
  built_ci_i386 = null;
  built_ci_lint = null;
  built_ci_cortexm = null;
  built_ci_wasm = null;

// Global variable assigned during Sanity Check that holds the sha1 which should be
// merged into the PR in all branches.
upstream_revision = null

// command to start a docker container
docker_run = 'docker/bash.sh'
// timeout in minutes
max_time = 240

// Filenames for stashing between build and test steps
s3_prefix = "tvm-jenkins-artifacts-prod/tvm/${env.BRANCH_NAME}/${env.BUILD_NUMBER}"


// General note: Jenkins has limits on the size of a method (or top level code)
// that are pretty strict, so most usage of groovy methods in these templates
// are purely to satisfy the JVM
def per_exec_ws(folder) {
  return "workspace/exec_${env.EXECUTOR_NUMBER}/" + folder
}

// initialize source codes
def init_git() {
  checkout scm


  // Add more info about job node
  sh (
    script: './tests/scripts/task_show_node_info.sh',
    label: 'Show executor node info',
  )

  // Determine merge commit to use for all stages
  sh (
    script: 'git fetch origin main',
    label: 'Fetch upstream',
  )
  if (upstream_revision == null) {
    upstream_revision = sh(
      script: 'git log -1 FETCH_HEAD --format=\'%H\'',
      label: 'Determine upstream revision',
      returnStdout: true,
    ).trim()
  }
  sh (
    script: "git -c user.name=TVM-Jenkins -c user.email=jenkins@tvm.apache.org merge ${upstream_revision}",
    label: 'Merge to origin/main'
  )

  sh(
    script: """
      set -eux
      retry() {
  local max_retries=\$1
  shift
  local n=0
  local backoff_max=30
  until [ "\$n" -ge \$max_retries ]
  do
      "\$@" && break
      n=\$((n+1))
      if [ "\$n" -eq \$max_retries ]; then
          echo "failed to update after attempt \$n / \$max_retries, giving up"
          exit 1
      fi

      WAIT=\$(python3 -c 'import random; print(random.randint(10, 30))')
      echo "failed to update \$n / \$max_retries, waiting \$WAIT to try again"
      sleep \$WAIT
  done
}

      retry 3 timeout 5m git submodule update --init -f --jobs 0
    """,
    label: 'Update git submodules',
  )
}

def docker_init(image) {
  // Clear out all Docker images that aren't going to be used
  sh(
    script: """
    set -eux
    docker image ls --all
    IMAGES=\$(docker image ls --all --format '{{.Repository}}:{{.Tag}}  {{.ID}}')

    echo -e "Found images:\\n\$IMAGES"
    echo "\$IMAGES" | { grep -vE '${image}' || test \$? = 1; } | { xargs docker rmi || test \$? = 123; }

    docker image ls --all
    """,
    label: 'Clean old Docker images',
  )

  if (image.contains("amazonaws.com")) {
    // If this string is in the image name it's from ECR and needs to be pulled
    // with the right credentials
    ecr_pull(image)
  } else {
    sh(
      script: """
      set -eux
      retry() {
  local max_retries=\$1
  shift
  local n=0
  local backoff_max=30
  until [ "\$n" -ge \$max_retries ]
  do
      "\$@" && break
      n=\$((n+1))
      if [ "\$n" -eq \$max_retries ]; then
          echo "failed to update after attempt \$n / \$max_retries, giving up"
          exit 1
      fi

      WAIT=\$(python3 -c 'import random; print(random.randint(10, 30))')
      echo "failed to update \$n / \$max_retries, waiting \$WAIT to try again"
      sleep \$WAIT
  done
}

      retry 3 docker pull ${image}
      """,
      label: 'Pull docker image',
    )
  }
}

def should_skip_slow_tests(pr_number) {
  withCredentials([string(
    credentialsId: 'tvm-bot-jenkins-reader',
    variable: 'GITHUB_TOKEN',
  )]) {
    // Exit code of 1 means run slow tests, exit code of 0 means skip slow tests
    result = sh (
      returnStatus: true,
      script: "./tests/scripts/should_run_slow_tests.py --pr '${pr_number}'",
      label: 'Check if CI should run slow tests',
    )
  }
  return result == 0
}

def cancel_previous_build() {
  // cancel previous build if it is not on main.
  if (env.BRANCH_NAME != 'main') {
    def buildNumber = env.BUILD_NUMBER as int
    // Milestone API allows us to cancel previous build
    // with the same milestone number
    if (buildNumber > 1) milestone(buildNumber - 1)
    milestone(buildNumber)
  }
}

def should_skip_ci(pr_number) {
  withCredentials([string(
    credentialsId: 'tvm-bot-jenkins-reader',
    variable: 'TOKEN',
    )]) {
    // Exit code of 1 means run full CI (or the script had an error, so run
    // full CI just in case). Exit code of 0 means skip CI.
    git_skip_ci_code = sh (
      returnStatus: true,
      script: "./tests/scripts/git_skip_ci.py --pr '${pr_number}'",
      label: 'Check if CI should be skipped',
    )
    }
  return git_skip_ci_code == 0
}

cancel_previous_build()

def lint() {
stage('Lint') {
  node('CPU') {
    timeout(time: max_time, unit: 'MINUTES') {
      ci_lint = params.ci_lint_param ?: ci_lint
      ci_cpu = params.ci_cpu_param ?: ci_cpu
      ci_gpu = params.ci_gpu_param ?: ci_gpu
      ci_wasm = params.ci_wasm_param ?: ci_wasm
      ci_i386 = params.ci_i386_param ?: ci_i386
      ci_qemu = params.ci_qemu_param ?: ci_qemu
      ci_arm = params.ci_arm_param ?: ci_arm
      ci_hexagon = params.ci_hexagon_param ?: ci_hexagon

      sh (script: """
        echo "Docker images being used in this build:"
        echo " ci_lint = ${ci_lint}"
        echo " ci_cpu  = ${ci_cpu}"
        echo " ci_gpu  = ${ci_gpu}"
        echo " ci_wasm = ${ci_wasm}"
        echo " ci_i386 = ${ci_i386}"
        echo " ci_qemu = ${ci_qemu}"
        echo " ci_arm  = ${ci_arm}"
        echo " ci_hexagon  = ${ci_hexagon}"
      """, label: 'Docker image names')

      ws("workspace/exec_${env.EXECUTOR_NUMBER}/tvm/sanity") {
        init_git()

        if (env.DETERMINE_DOCKER_IMAGES == 'yes') {
          sh(
            script: "./tests/scripts/determine_docker_images.py ci_arm=${ci_arm} ci_cpu=${ci_cpu} ci_minimal=${ci_minimal} ci_gpu=${ci_gpu} ci_hexagon=${ci_hexagon} ci_i386=${ci_i386} ci_lint=${ci_lint} ci_cortexm=${ci_cortexm} ci_wasm=${ci_wasm} ",
            label: 'Decide whether to use tlcpack or tlcpackstaging for Docker images',
          )
          // Pull image names from the results of should_rebuild_docker.py
          ci_arm = sh(
            script: "cat .docker-image-names/ci_arm",
            label: "Find docker image name for ci_arm",
            returnStdout: true,
          ).trim()
          ci_cpu = sh(
            script: "cat .docker-image-names/ci_cpu",
            label: "Find docker image name for ci_cpu",
            returnStdout: true,
          ).trim()
          ci_minimal = sh(
            script: "cat .docker-image-names/ci_minimal",
            label: "Find docker image name for ci_minimal",
            returnStdout: true,
          ).trim()
          ci_gpu = sh(
            script: "cat .docker-image-names/ci_gpu",
            label: "Find docker image name for ci_gpu",
            returnStdout: true,
          ).trim()
          ci_hexagon = sh(
            script: "cat .docker-image-names/ci_hexagon",
            label: "Find docker image name for ci_hexagon",
            returnStdout: true,
          ).trim()
          ci_i386 = sh(
            script: "cat .docker-image-names/ci_i386",
            label: "Find docker image name for ci_i386",
            returnStdout: true,
          ).trim()
          ci_lint = sh(
            script: "cat .docker-image-names/ci_lint",
            label: "Find docker image name for ci_lint",
            returnStdout: true,
          ).trim()
          ci_cortexm = sh(
            script: "cat .docker-image-names/ci_cortexm",
            label: "Find docker image name for ci_cortexm",
            returnStdout: true,
          ).trim()
          ci_wasm = sh(
            script: "cat .docker-image-names/ci_wasm",
            label: "Find docker image name for ci_wasm",
            returnStdout: true,
          ).trim()
        }

        ci_arm = params.ci_arm_param ?: ci_arm
        ci_cpu = params.ci_cpu_param ?: ci_cpu
        ci_minimal = params.ci_minimal_param ?: ci_minimal
        ci_gpu = params.ci_gpu_param ?: ci_gpu
        ci_hexagon = params.ci_hexagon_param ?: ci_hexagon
        ci_i386 = params.ci_i386_param ?: ci_i386
        ci_lint = params.ci_lint_param ?: ci_lint
        ci_cortexm = params.ci_cortexm_param ?: ci_cortexm
        ci_wasm = params.ci_wasm_param ?: ci_wasm

        sh (script: """
          echo "Docker images being used in this build:"
          echo " ci_arm = ${ci_arm}"
          echo " ci_cpu = ${ci_cpu}"
          echo " ci_minimal = ${ci_minimal}"
          echo " ci_gpu = ${ci_gpu}"
          echo " ci_hexagon = ${ci_hexagon}"
          echo " ci_i386 = ${ci_i386}"
          echo " ci_lint = ${ci_lint}"
          echo " ci_cortexm = ${ci_cortexm}"
          echo " ci_wasm = ${ci_wasm}"
        """, label: 'Docker image names')

        is_docs_only_build = sh (
          returnStatus: true,
          script: './tests/scripts/git_change_docs.sh',
          label: 'Check for docs only changes',
        )
        skip_ci = should_skip_ci(env.CHANGE_ID)
        skip_slow_tests = should_skip_slow_tests(env.CHANGE_ID)
        sh (
          script: "${docker_run} ${ci_lint}  ./tests/scripts/task_lint.sh",
          label: 'Run lint',
        )
      }
    }
  }
}
}

// [note: method size]
// This has to be extracted into a method due to JVM limitations on the size of
// a method (so the code can't all be inlined)
lint()

// Run make. First try to do an incremental make from a previous workspace in hope to
// accelerate the compilation. If something is wrong, clean the workspace and then
// build from scratch.
def make(docker_type, path, make_flag) {
  timeout(time: max_time, unit: 'MINUTES') {
    try {
      cmake_build(docker_type, path, make_flag)
      // always run cpp test when build
      // sh "${docker_run} ${docker_type} ./tests/scripts/task_cpp_unittest.sh"
    } catch (hudson.AbortException ae) {
      // script exited due to user abort, directly throw instead of retry
      if (ae.getMessage().contains('script returned exit code 143')) {
        throw ae
      }
      echo 'Incremental compilation failed. Fall back to build from scratch'
      sh (
        script: "${docker_run} ${docker_type} ./tests/scripts/task_clean.sh ${path}",
        label: 'Clear old cmake workspace',
      )
      cmake_build(docker_type, path, make_flag)
      cpp_unittest(docker_type)
    }
  },
    )
  }
}
def ci_setup(image) {
  sh (
    script: "${docker_run} ${image} ./tests/scripts/task_ci_setup.sh",
    label: 'Set up CI environment',
  )
}

def python_unittest(image) {
  sh (
    script: "${docker_run} ${image} ./tests/scripts/task_python_unittest.sh",
    label: 'Run Python unit tests',
  )
}

def fsim_test(image) {
  sh (
    script: "${docker_run} ${image} ./tests/scripts/task_python_vta_fsim.sh",
    label: 'Run VTA tests in FSIM',
  )
}

def cmake_build(image, path, make_flag) {
  sh (
    script: "${docker_run} --env CI_NUM_EXECUTORS ${image} ./tests/scripts/task_build.py --sccache-bucket tvm-sccache-prod",
    label: 'Run cmake build',
  )
}

def cpp_unittest(image) {
  sh (
    script: "${docker_run} ${image} ./tests/scripts/task_cpp_unittest.sh",
    label: 'Build and run C++ tests',
  )
}

def add_hexagon_permissions() {
  sh(
    script: 'find build/hexagon_api_output -type f | xargs chmod +x',
    label: 'Add execute permissions for hexagon files',
  )
}

stage('Build and Test') {
  if (is_docs_only_build != 1) {
    parallel 'BUILD: GPU': {
      node('GPU') {
        ws(per_exec_ws('tvm/build-gpu')) {
          init_git()
          sh "${docker_run} ${ci_gpu} nvidia-smi"
          sh "${docker_run}  ${ci_gpu} ./tests/scripts/task_config_build_gpu.sh build"
          make("${ci_gpu}", 'build', '-j2')
          sh "${docker_run} ${ci_gpu} ./tests/scripts/task_python_integration_gpuonly.sh"
        }
      }
    },
    'BUILD: CPU': {
      node('CPU') {
        ws(per_exec_ws('tvm/build-cpu')) {
          init_git()
          sh "${docker_run} ${ci_cpu} ./tests/scripts/task_config_build_cpu.sh build"
          make(ci_cpu, 'build', '-j2')
          sh "${docker_run} ${ci_cpu} ./tests/scripts/task_python_integration.sh"
        }
      }
    }
  } else {
    Utils.markStageSkippedForConditional('BUILD: CPU')
  }
}

// stage('Build') {
//     parallel 'BUILD: GPU': {
//       node('GPUBUILD') {
//         ws(per_exec_ws('tvm/build-gpu')) {
//           init_git()
//           sh "${docker_run} ${ci_gpu} ./tests/scripts/task_config_build_gpu.sh build"
//           make(ci_gpu, 'build', '-j2')
//           pack_lib('gpu', tvm_multilib)
//           // compiler test
//           sh "${docker_run} ${ci_gpu} ./tests/scripts/task_config_build_gpu_other.sh build2"
//           make(ci_gpu, 'build2', '-j2')
//       }
//     }
//   },
//   'BUILD: CPU': {
//     if (is_docs_only_build != 1) {
//       node('CPU') {
//         ws(per_exec_ws('tvm/build-cpu')) {
//           init_git()
//           sh "${docker_run} ${ci_cpu} ./tests/scripts/task_config_build_cpu.sh build"
//           make(ci_cpu, 'build', '-j2')
//           pack_lib('cpu', tvm_multilib_tsim)
//           timeout(time: max_time, unit: 'MINUTES') {
//             sh "${docker_run} ${ci_cpu} ./tests/scripts/task_ci_setup.sh"
//             sh "${docker_run} ${ci_cpu} ./tests/scripts/task_python_unittest.sh"
//             sh "${docker_run} ${ci_cpu} ./tests/scripts/task_python_vta_fsim.sh"
//             sh "${docker_run} ${ci_cpu} ./tests/scripts/task_python_vta_tsim.sh"
//             // sh "${docker_run} ${ci_cpu} ./tests/scripts/task_golang.sh"
//             // TODO(@jroesch): need to resolve CI issue will turn back on in follow up patch
//             sh "${docker_run} ${ci_cpu} ./tests/scripts/task_rust.sh"
//             junit "build/pytest-results/*.xml"
//           }
//         }
//       }
//     } else {
//       Utils.markStageSkippedForConditional('BUILD: CPU')
//     }
//   },
//   'BUILD: WASM': {
//     if (is_docs_only_build != 1) {
//       node('CPU') {
//         ws(per_exec_ws('tvm/build-wasm')) {
//           init_git()
//           sh "${docker_run} ${ci_wasm} ./tests/scripts/task_config_build_wasm.sh build"
//           make(ci_wasm, 'build', '-j2')
//           timeout(time: max_time, unit: 'MINUTES') {
//             sh "${docker_run} ${ci_wasm} ./tests/scripts/task_ci_setup.sh"
//             sh "${docker_run} ${ci_wasm} ./tests/scripts/task_web_wasm.sh"
//           }
//         }
//       }
//     } else {
//       Utils.markStageSkippedForConditional('BUILD: WASM')
//     }
//   },
//   'BUILD : i386': {
//     if ( is_docs_only_build != 1) {
//       node('CPU') {
//         ws(per_exec_ws('tvm/build-i386')) {
//           init_git()
//           sh "${docker_run} ${ci_i386} ./tests/scripts/task_config_build_i386.sh build"
//           make(ci_i386, 'build', '-j2')
//           pack_lib('i386', tvm_multilib_tsim)
//         }
//       }
//     } else {
//       Utils.markStageSkippedForConditional('BUILD : i386')
//     }
//   },
//   'BUILD : arm': {
//     if (is_docs_only_build != 1) {
//       node('ARM') {
//         ws(per_exec_ws('tvm/build-arm')) {
//           init_git()
//           sh "${docker_run} ${ci_arm} ./tests/scripts/task_config_build_arm.sh build"
//           make(ci_arm, 'build', '-j4')
//           pack_lib('arm', tvm_multilib)
//         }
//       }
//      } else {
//       Utils.markStageSkippedForConditional('BUILD : arm')
//     }
//   },
//   'BUILD: QEMU': {
//     if (is_docs_only_build != 1) {
//       node('CPU') {
//         ws(per_exec_ws('tvm/build-qemu')) {
//           init_git()
//           sh "${docker_run} ${ci_qemu} ./tests/scripts/task_config_build_qemu.sh build"
//           make(ci_qemu, 'build', '-j2')
//           timeout(time: max_time, unit: 'MINUTES') {
//             sh "${docker_run} ${ci_qemu} ./tests/scripts/task_ci_setup.sh"
//             sh "${docker_run} ${ci_qemu} ./tests/scripts/task_python_microtvm.sh"
//             junit "build/pytest-results/*.xml"
//           }
//         }
//       }
//      } else {
//       Utils.markStageSkippedForConditional('BUILD: QEMU')
//     }
//   }
// }

// stage('Unit Test') {
//     parallel 'python3: GPU': {
//       if (is_docs_only_build != 1) {
//         node('TensorCore') {
//           ws(per_exec_ws('tvm/ut-python-gpu')) {
//             init_git()
//             unpack_lib('gpu', tvm_multilib)
//             timeout(time: max_time, unit: 'MINUTES') {
//               sh "${docker_run} ${ci_gpu} ./tests/scripts/task_ci_setup.sh"
//               sh "${docker_run} ${ci_gpu} ./tests/scripts/task_sphinx_precheck.sh"
//               sh "${docker_run} ${ci_gpu} ./tests/scripts/task_python_unittest_gpuonly.sh"
//               sh "${docker_run} ${ci_gpu} ./tests/scripts/task_python_integration_gpuonly.sh"
//               junit "build/pytest-results/*.xml"
//             }
//           }
//         }
//       } else {
//         Utils.markStageSkippedForConditional('python3: i386')
//       }
//     },
//     'python3: CPU': {
//       if (is_docs_only_build != 1) {
//         node('CPU') {
//           ws(per_exec_ws("tvm/ut-python-cpu")) {
//             init_git()
//             unpack_lib('cpu', tvm_multilib_tsim)
//             timeout(time: max_time, unit: 'MINUTES') {
//               sh "${docker_run} ${ci_cpu} ./tests/scripts/task_ci_setup.sh"
//               sh "${docker_run} ${ci_cpu} ./tests/scripts/task_python_integration.sh"
//               junit "build/pytest-results/*.xml"
//             }
//           }
//         }
//       } else {
//         Utils.markStageSkippedForConditional('python3: i386')
//       }
//     },
//     'python3: i386': {
//       if (is_docs_only_build != 1) {
//         node('CPU') {
//           ws(per_exec_ws('tvm/ut-python-i386')) {
//             init_git()
//             unpack_lib('i386', tvm_multilib)
//             timeout(time: max_time, unit: 'MINUTES') {
//               sh "${docker_run} ${ci_i386} ./tests/scripts/task_ci_setup.sh"
//               sh "${docker_run} ${ci_i386} ./tests/scripts/task_python_unittest.sh"
//               sh "${docker_run} ${ci_i386} ./tests/scripts/task_python_integration_i386only.sh"
//               sh "${docker_run} ${ci_i386} ./tests/scripts/task_python_vta_fsim.sh"
//               junit "build/pytest-results/*.xml"
//             }
//           }
//         }
//      } else {
//         Utils.markStageSkippedForConditional('python3: i386')
//       }
//     },
//     'python3: arm': {
//       if (is_docs_only_build != 1) {
//         node('ARM') {
//           ws(per_exec_ws('tvm/ut-python-arm')) {
//             init_git()
//             unpack_lib('arm', tvm_multilib)
//             timeout(time: max_time, unit: 'MINUTES') {
//               sh "${docker_run} ${ci_arm} ./tests/scripts/task_ci_setup.sh"
//               sh "${docker_run} ${ci_arm} ./tests/scripts/task_python_unittest.sh"
//               sh "${docker_run} ${ci_arm} ./tests/scripts/task_python_arm_compute_library.sh"
//               junit "build/pytest-results/*.xml"
//             // sh "${docker_run} ${ci_arm} ./tests/scripts/task_python_integration.sh"
//             }
//           }
//         }
//       } else {
//          Utils.markStageSkippedForConditional('python3: arm')
//       }
//     },
//     'java: GPU': {
//       if (is_docs_only_build != 1 ) {
//         node('GPU') {
//           ws(per_exec_ws('tvm/ut-java')) {
//             init_git()
//               unpack_lib('gpu', tvm_multilib)
//               timeout(time: max_time, unit: 'MINUTES') {
//                 sh "${docker_run} ${ci_gpu} ./tests/scripts/task_ci_setup.sh"
//                 sh "${docker_run} ${ci_gpu} ./tests/scripts/task_java_unittest.sh"
//               }
//           }
//         }
//       } else {
//          Utils.markStageSkippedForConditional('java: GPU')
//       }
//     }
// }

// stage('Integration Test') {
//   parallel 'topi: GPU': {
//   if (is_docs_only_build != 1) {
//     node('GPU') {
//       ws(per_exec_ws('tvm/topi-python-gpu')) {
//         init_git()
//         unpack_lib('gpu', tvm_multilib)
//         timeout(time: max_time, unit: 'MINUTES') {
//           sh "${docker_run} ${ci_gpu} ./tests/scripts/task_ci_setup.sh"
//           sh "${docker_run} ${ci_gpu} ./tests/scripts/task_python_topi.sh"
//           junit "build/pytest-results/*.xml"
//         }
//       }
//     }
//     } else {
//       Utils.markStageSkippedForConditional('topi: GPU')
//   }
//   },
//   'frontend: GPU': {
//     if (is_docs_only_build != 1) {
//       node('GPU') {
//         ws(per_exec_ws('tvm/frontend-python-gpu')) {
//           init_git()
//           unpack_lib('gpu', tvm_multilib)
//           timeout(time: max_time, unit: 'MINUTES') {
//             sh "${docker_run} ${ci_gpu} ./tests/scripts/task_ci_setup.sh"
//             sh "${docker_run} ${ci_gpu} ./tests/scripts/task_python_frontend.sh"
//             junit "build/pytest-results/*.xml"
//           }
//         }
//       }
//      } else {
//       Utils.markStageSkippedForConditional('frontend: GPU')
//     }
//   },
//   'frontend: CPU': {
//     if (is_docs_only_build != 1) {
//       node('CPU') {
//         ws(per_exec_ws('tvm/frontend-python-cpu')) {
//           init_git()
//           unpack_lib('cpu', tvm_multilib)
//           timeout(time: max_time, unit: 'MINUTES') {
//             sh "${docker_run} ${ci_cpu} ./tests/scripts/task_ci_setup.sh"
//             sh "${docker_run} ${ci_cpu} ./tests/scripts/task_python_frontend_cpu.sh"
//             junit "build/pytest-results/*.xml"
//           }
//         }
//       }
//     } else {
//       Utils.markStageSkippedForConditional('frontend: CPU')
//     }
//   },
//   'docs: GPU': {
//     node('TensorCore') {
//       ws(per_exec_ws('tvm/docs-python-gpu')) {
//         init_git()
//         unpack_lib('gpu', tvm_multilib)
//         timeout(time: max_time, unit: 'MINUTES') {
//           sh "${docker_run} ${ci_gpu} ./tests/scripts/task_ci_setup.sh"
//           sh "${docker_run} ${ci_gpu} ./tests/scripts/task_python_docs.sh"
//         }
//         pack_lib('mydocs', 'docs.tgz')
//       }
//     }
//   }
// }

// /*
// stage('Build packages') {
//   parallel 'conda CPU': {
//     node('CPU') {
//       sh "${docker_run} tlcpack/conda-cpu ./conda/build_cpu.sh
//     }
//   },
//   'conda cuda': {
//     node('CPU') {
//       sh "${docker_run} tlcpack/conda-cuda90 ./conda/build_cuda.sh
//       sh "${docker_run} tlcpack/conda-cuda100 ./conda/build_cuda.sh
//     }
//   }
// // Here we could upload the packages to anaconda for releases
// // and/or the main branch
// }
// */

// stage('Deploy') {
//     node('doc') {
//       ws(per_exec_ws('tvm/deploy-docs')) {
//         if (env.BRANCH_NAME == 'main') {
//         unpack_lib('mydocs', 'docs.tgz')
//         sh 'cp docs.tgz /var/docs/docs.tgz'
//         sh 'tar xf docs.tgz -C /var/docs'
//         }
//       }
//     }
// }